{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa53da25",
   "metadata": {},
   "source": [
    "PART B: DEEP LEARNING VIDEO CLASSIFICATION (REAL DATA)\n",
    "\n",
    "This script implements:\n",
    "1. 2D CNN (ResNet-18) + Temporal Pooling\n",
    "2. 3D CNN (R(2+1)D-18)\n",
    "\n",
    "Dataset:\n",
    "- UCF-style directory\n",
    "- Predefined train/test splits\n",
    "\n",
    "Evaluation:\n",
    "- Accuracy\n",
    "- Precision (macro)\n",
    "- Recall (macro)\n",
    "- F1-score (macro)\n",
    "- Confusion Matrix\n",
    "- Training time\n",
    "- Inference time per video\n",
    "\n",
    "Author: 2024ab05275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# IMPORTS\n",
    "# ==========================================================\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9008d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================\n",
    "# Current file is inside /code\n",
    "CODE_DIR = Path.cwd()\n",
    "\n",
    "# Project root is one level above\n",
    "PROJECT_ROOT = CODE_DIR.parent\n",
    "\n",
    "DATASET_ROOT = PROJECT_ROOT / \"dataset\"\n",
    "SPLITS_DIR = DATASET_ROOT / \"splits\"\n",
    "\n",
    "LOCAL_WEIGHTS = os.path.join(PROJECT_ROOT, \"model\", \"resnet18-f37072fd.pth\")\n",
    "\n",
    "# Safety checks (VERY IMPORTANT)\n",
    "assert DATASET_ROOT.exists(), f\"Dataset not found at {DATASET_ROOT}\"\n",
    "assert SPLITS_DIR.exists(), f\"Splits folder not found at {SPLITS_DIR}\"\n",
    "\n",
    "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Dataset root: {DATASET_ROOT}\")\n",
    "\n",
    "NUM_FRAMES = 16                 # Frames sampled per video\n",
    "IMG_SIZE = (224, 224)           # Required for pretrained CNNs\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nðŸš€ Running on device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# CLASS MAPPING (Derived from folder names)\n",
    "# ==========================================================\n",
    "CLASS_NAMES = sorted([\n",
    "    d.name for d in DATASET_ROOT.iterdir()\n",
    "    if d.is_dir() and d.name.startswith(\"class_\")\n",
    "])\n",
    "\n",
    "CLASS_TO_IDX = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(\"\\nðŸ“Œ Class mapping:\")\n",
    "for k, v in CLASS_TO_IDX.items():\n",
    "    print(f\"  {k} â†’ {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a358024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# VIDEO PREPROCESSING\n",
    "# ==========================================================\n",
    "\"\"\"\n",
    "- OpenCV used for video loading\n",
    "- Uniform frame sampling\n",
    "- Resize to 224Ã—224\n",
    "- ImageNet normalization (mandatory for pretrained models)\n",
    "\"\"\"\n",
    "\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def load_video(video_path, num_frames=NUM_FRAMES):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, IMG_SIZE)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Uniform temporal sampling\n",
    "    if len(frames) >= num_frames:\n",
    "        idx = np.linspace(0, len(frames) - 1, num_frames).astype(int)\n",
    "        frames = [frames[i] for i in idx]\n",
    "    else:\n",
    "        while len(frames) < num_frames:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "    frames = [imagenet_transform(f) for f in frames]\n",
    "    return torch.stack(frames)  # (T, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# LOAD DATA USING OFFICIAL SPLITS\n",
    "# ==========================================================\n",
    "def load_split(split_file):\n",
    "    videos, labels = [], []\n",
    "\n",
    "    with open(split_file, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        video_path = DATASET_ROOT / line\n",
    "        class_name = line.split(\"/\")[0]\n",
    "        label = CLASS_TO_IDX[class_name]\n",
    "\n",
    "        videos.append(load_video(video_path))\n",
    "        labels.append(label)\n",
    "\n",
    "    return torch.stack(videos), torch.tensor(labels)\n",
    "\n",
    "print(\"\\nðŸ“‚ Loading training data...\")\n",
    "X_train, y_train = load_split(SPLITS_DIR / \"train.txt\")\n",
    "\n",
    "print(\"ðŸ“‚ Loading testing data...\")\n",
    "X_test, y_test = load_split(SPLITS_DIR / \"test.txt\")\n",
    "\n",
    "print(\"\\nâœ… Dataset summary:\")\n",
    "print(f\"  Training videos: {len(X_train)}\")\n",
    "print(f\"  Testing videos : {len(X_test)}\")\n",
    "print(f\"  Classes        : {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# DATASET CLASS WITH AUGMENTATION\n",
    "# ==========================================================\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Applies spatial augmentation only during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, train=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.train = train\n",
    "        self.flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = self.X[idx]\n",
    "        if self.train:\n",
    "            video = torch.stack([self.flip(f) for f in video])\n",
    "        return video, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# DATALOADERS\n",
    "# ==========================================================\n",
    "train_loader = DataLoader(\n",
    "    VideoDataset(X_train, y_train, train=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    VideoDataset(X_test, y_test, train=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Data loaders initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# MODEL DEFINITIONS\n",
    "# ==========================================================\n",
    "# Path to your downloaded weights\n",
    "\n",
    "print(f\"* Local Weight Path : {LOCAL_WEIGHTS}\")\n",
    "\n",
    "class CNN2DTemporal(nn.Module):\n",
    "    def __init__(self, num_classes, local_weights_path=LOCAL_WEIGHTS):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load ResNet18 without downloading\n",
    "        base = models.resnet18(weights=None)\n",
    "        # Load local pretrained weights\n",
    "        if os.path.exists(local_weights_path):\n",
    "            print(f\"Loading ResNet18 weights from {local_weights_path}\")\n",
    "            state_dict = torch.load(local_weights_path, map_location=\"cpu\")\n",
    "            base.load_state_dict(state_dict)\n",
    "        else:\n",
    "            print(\"âš ï¸ Local weights not found, initializing randomly\")\n",
    "\n",
    "        # Remove classifier\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.backbone(x).view(B, T, -1)\n",
    "        pooled = feats.mean(dim=1) + feats.max(dim=1)[0]\n",
    "        return self.fc(pooled)\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3D CNN explicitly models spatiotemporal features\n",
    "    using 3D convolutions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.video.r2plus1d_18(pretrained=True)\n",
    "        self.model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af423aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# TRAINING & EVALUATION FUNCTION\n",
    "# ==========================================================\n",
    "def train_and_evaluate(model, model_name):\n",
    "    print(f\"\\nðŸš€ Training {model_name}\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for videos, labels in train_loader:\n",
    "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(videos), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    start_inf = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in test_loader:\n",
    "            videos = videos.to(DEVICE)\n",
    "            outputs = model(videos)\n",
    "            preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "\n",
    "    inf_time = (time.time() - start_inf) / len(targets)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(targets, preds),\n",
    "        \"precision\": precision_score(targets, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(targets, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(targets, preds, average=\"macro\"),\n",
    "        \"train_time\": train_time,\n",
    "        \"inf_time\": inf_time,\n",
    "        \"cm\": confusion_matrix(targets, preds)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ“Š {model_name} Performance\")\n",
    "    for k, v in metrics.items():\n",
    "        if k not in [\"cm\"]:\n",
    "            print(f\"{k.replace('_',' ').title():<20}: {v:.4f}\")\n",
    "\n",
    "    sns.heatmap(metrics[\"cm\"], annot=True, fmt=\"d\")\n",
    "    plt.title(model_name)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe317536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# RUN EXPERIMENTS\n",
    "# ==========================================================\n",
    "metrics_2d = train_and_evaluate(\n",
    "    CNN2DTemporal(NUM_CLASSES),\n",
    "    \"2D CNN + Temporal Pooling\"\n",
    ")\n",
    "\n",
    "metrics_3d = train_and_evaluate(\n",
    "    CNN3D(NUM_CLASSES),\n",
    "    \"3D CNN (R(2+1)D)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project root: /Users/chocalingamlakshmanan/Desktop/Video-analytics-assignment/Student_2024ab05275_Video_Classification\n",
      "âœ… Dataset root: /Users/chocalingamlakshmanan/Desktop/Video-analytics-assignment/Student_2024ab05275_Video_Classification/dataset\n",
      "\n",
      "ðŸš€ Running on device: cpu\n",
      "\n",
      "ðŸ“Œ Class mapping:\n",
      "  class_1_Basketball â†’ 0\n",
      "  class_2_Biking â†’ 1\n",
      "  class_3_WalkingWithDog â†’ 2\n",
      "\n",
      "ðŸ“‚ Loading training data...\n",
      "ðŸ“‚ Loading testing data...\n",
      "\n",
      "âœ… Dataset summary:\n",
      "  Training videos: 106\n",
      "  Testing videos : 24\n",
      "  Classes        : 3\n",
      "\n",
      "âœ… Data loaders initialized\n",
      "* Local Weight Path : model/resnet18-f37072fd.pth\n",
      "âš ï¸ Local weights not found, initializing randomly\n",
      "\n",
      "ðŸš€ Training 2D CNN + Temporal Pooling\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 312\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# RUN EXPERIMENTS\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m metrics_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCNN2DTemporal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2D CNN + Temporal Pooling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    315\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m metrics_3d \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m    318\u001b[0m     CNN3D(NUM_CLASSES),\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3D CNN (R(2+1)D)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# DETAILED COMPARISON REPORT\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 265\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, model_name)\u001b[0m\n\u001b[1;32m    263\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    264\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(model(videos), labels)\n\u001b[0;32m--> 265\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# DETAILED COMPARISON REPORT\n",
    "# ==========================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ˆ DETAILED MODEL COMPARISON REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ”¹ Accuracy Comparison\")\n",
    "print(f\"2D CNN Accuracy : {metrics_2d['accuracy']:.4f}\")\n",
    "print(f\"3D CNN Accuracy : {metrics_3d['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Computational Efficiency\")\n",
    "print(f\"2D CNN Training Time (s): {metrics_2d['train_time']:.2f}\")\n",
    "print(f\"3D CNN Training Time (s): {metrics_3d['train_time']:.2f}\")\n",
    "print(f\"2D CNN Inference Time / Video (s): {metrics_2d['inf_time']:.4f}\")\n",
    "print(f\"3D CNN Inference Time / Video (s): {metrics_3d['inf_time']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Qualitative Analysis\")\n",
    "if metrics_3d[\"accuracy\"] > metrics_2d[\"accuracy\"]:\n",
    "    print(\n",
    "        \"âœ” The 3D CNN outperforms the 2D CNN by explicitly modeling \"\n",
    "        \"spatiotemporal patterns, making it more suitable for complex actions.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"âœ” The 2D CNN achieves competitive accuracy with significantly lower \"\n",
    "        \"computational cost, making it suitable for real-time applications.\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    \"\\nðŸ”¹ Final Conclusion:\\n\"\n",
    "    \"2D CNNs provide a strong baseline with efficient inference, while \"\n",
    "    \"3D CNNs offer improved performance at the cost of higher computation.\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Experiment completed successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
