{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa53da25",
   "metadata": {},
   "source": [
    "PART B: DEEP LEARNING VIDEO CLASSIFICATION (REAL DATA)\n",
    "\n",
    "This script implements:\n",
    "1. 2D CNN (ResNet-18) + Temporal Pooling\n",
    "2. 3D CNN (R(2+1)D-18)\n",
    "\n",
    "Dataset:\n",
    "- UCF-style directory\n",
    "- Predefined train/test splits\n",
    "\n",
    "Evaluation:\n",
    "- Accuracy\n",
    "- Precision (macro)\n",
    "- Recall (macro)\n",
    "- F1-score (macro)\n",
    "- Confusion Matrix\n",
    "- Training time\n",
    "- Inference time per video\n",
    "\n",
    "Author: 2024ab05275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7155a69f-1727-4343-8b54-45bfac7cbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.11.3)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.3.0+cu121)\n",
      "Requirement already satisfied: torchvision>=0.15 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.18.0+cu121)\n",
      "Collecting opencv-python-headless (from -r requirements.txt (line 10))\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.22.0)\n",
      "Requirement already satisfied: matplotlib>=3.7 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (3.8.4)\n",
      "Requirement already satisfied: seaborn>=0.13 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.66 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision>=0.15->-r requirements.txt (line 7)) (10.4.0)\n",
      "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.3->-r requirements.txt (line 11)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.3->-r requirements.txt (line 11)) (3.5.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (2024.7.2)\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (24.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (2.9.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn>=0.13->-r requirements.txt (line 16)) (2.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn>=0.13->-r requirements.txt (line 16)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn>=0.13->-r requirements.txt (line 16)) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.7->-r requirements.txt (line 15)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf86cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# IMPORTS\n",
    "# ==========================================================\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Standard Library Imports\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import os\n",
    "# File-system operations such as path checks and environment handling\n",
    "\n",
    "import time\n",
    "# Execution time measurement and performance benchmarking\n",
    "\n",
    "from pathlib import Path\n",
    "# Object-oriented and platform-independent file path management\n",
    "\n",
    "from typing import List, Tuple\n",
    "# Type annotations for improved readability and static analysis\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Scientific Computing & Computer Vision Libraries\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "# Numerical computation, array manipulation, and statistical operations\n",
    "\n",
    "import cv2\n",
    "# Video decoding, frame preprocessing, color-space conversion, and filtering\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PyTorch Core Framework\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "# Tensor computation, GPU acceleration, and automatic differentiation\n",
    "\n",
    "import torch.nn as nn\n",
    "# Neural network layer definitions and model construction primitives\n",
    "\n",
    "import torch.optim as optim\n",
    "# Optimization algorithms for training neural networks\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Dataset abstraction and efficient batch-wise data loading\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Torchvision Utilities\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "from torchvision import models, transforms\n",
    "# Pretrained CNN backbones and image preprocessing pipelines\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluation Metrics (Scikit-learn)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "# Classification performance metrics and confusion matrix analysis\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Visualization Libraries\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting utilities for training curves and comparative analysis\n",
    "\n",
    "import seaborn as sns\n",
    "# High-level statistical visualizations and heatmaps\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Project-Specific Model Architectures\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "from models import CNN2DTemporal, CNN3D\n",
    "# Custom deep learning models for video classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9008d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "âœ… Project root: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification\n",
      "âœ… Dataset root: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset\n",
      "âœ… Local Weight Path : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/model/resnet18-f37072fd.pth\n",
      "----------------------------------------------------------\n",
      "ğŸš€ Running on device: cuda\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================\n",
    "# Current file is inside /code\n",
    "CODE_DIR = Path.cwd()\n",
    "\n",
    "# Project root is one level above\n",
    "PROJECT_ROOT = CODE_DIR.parent\n",
    "\n",
    "DATASET_ROOT = PROJECT_ROOT / \"dataset\"\n",
    "SPLITS_DIR = DATASET_ROOT / \"splits\"\n",
    "\n",
    "LOCAL_WEIGHTS = os.path.join(PROJECT_ROOT, \"model\", \"resnet18-f37072fd.pth\")\n",
    "\n",
    "# Safety checks (VERY IMPORTANT)\n",
    "assert DATASET_ROOT.exists(), f\"Dataset not found at {DATASET_ROOT}\"\n",
    "assert SPLITS_DIR.exists(), f\"Splits folder not found at {SPLITS_DIR}\"\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Dataset root: {DATASET_ROOT}\")\n",
    "print(f\"âœ… Local Weight Path : {LOCAL_WEIGHTS}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "NUM_FRAMES = 16                 # Frames sampled per video\n",
    "IMG_SIZE = (224, 224)           # Required for pretrained CNNs\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸš€ Running on device: {DEVICE}\")\n",
    "print(\"----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73aa49f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Scanning dataset root for class folders.....\n",
      "* Dataset root path: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset\n",
      "----------------------------------------------------------\n",
      "* No of Class Folders Found : 3\n",
      "* Detected class folders (sorted):\n",
      "  - class_1_Basketball\n",
      "  - class_2_Biking\n",
      "  - class_3_WalkingWithDog\n",
      "----------------------------------------------------------\n",
      "* Final class-to-index mapping:\n",
      "  - class_1_Basketball â†’ 0\n",
      "  - class_2_Biking â†’ 1\n",
      "  - class_3_WalkingWithDog â†’ 2\n",
      "----------------------------------------------------------\n",
      "* Total number of classes: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CLASS MAPPING (Derived from dataset folder names)\n",
    "# ==========================================================\n",
    "# This section automatically discovers class labels from the\n",
    "# dataset directory structure.\n",
    "#\n",
    "# Expected directory format:\n",
    "# DATASET_ROOT/\n",
    "# â”œâ”€â”€ class_0/\n",
    "# â”œâ”€â”€ class_1/\n",
    "# â”œâ”€â”€ class_2/\n",
    "# â””â”€â”€ ...\n",
    "#\n",
    "# Each \"class_*\" folder represents one target class.\n",
    "# ==========================================================\n",
    "\n",
    "print(\"* Scanning dataset root for class folders.....\")\n",
    "print(f\"* Dataset root path: {DATASET_ROOT}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Discover class directory names\n",
    "# ----------------------------------------------------------\n",
    "# - Iterate over all items inside DATASET_ROOT\n",
    "# - Keep only directories\n",
    "# - Keep only directory names that start with \"class_\"\n",
    "# - Sort them to ensure consistent class index assignment\n",
    "# ----------------------------------------------------------\n",
    "CLASS_NAMES = sorted([\n",
    "    d.name                      # Folder name (e.g., \"class_1\")\n",
    "    for d in DATASET_ROOT.iterdir()\n",
    "    if d.is_dir()                # Ensure it is a directory\n",
    "    and d.name.startswith(\"class_\")  # Enforce naming convention\n",
    "])\n",
    "\n",
    "print(f\"* No of Class Folders Found : {len(CLASS_NAMES)}\")\n",
    "print(\"* Detected class folders (sorted):\")\n",
    "for cls in CLASS_NAMES:\n",
    "    print(f\"  - {cls}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Create class-to-index mapping\n",
    "# ----------------------------------------------------------\n",
    "# Assign a unique integer label to each class name.\n",
    "# The index order is determined by the sorted CLASS_NAMES list.\n",
    "#\n",
    "# Example:\n",
    "#   class_0 -> 0\n",
    "#   class_1 -> 1\n",
    "# ----------------------------------------------------------\n",
    "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Count total number of classes\n",
    "# ----------------------------------------------------------\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Log final class mapping\n",
    "# ----------------------------------------------------------\n",
    "print(\"* Final class-to-index mapping:\")\n",
    "for class_name, class_idx in CLASS_TO_IDX.items():\n",
    "    print(f\"  - {class_name} â†’ {class_idx}\")\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"* Total number of classes: {NUM_CLASSES}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a358024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¨ Initializing ImageNet preprocessing pipeline...\n",
      "ğŸ”„ Adding ToTensor():\n",
      "   - Converts NumPy/PIL image to torch.Tensor\n",
      "   - Reorders dimensions to (C, H, W)\n",
      "   - Scales pixel range to [0.0, 1.0]\n",
      "ğŸ“ Adding Normalize():\n",
      "   - Mean (RGB): [0.485, 0.456, 0.406]\n",
      "   - Std  (RGB): [0.229, 0.224, 0.225]\n",
      "ğŸ§© Composing preprocessing transforms\n",
      "âœ… ImageNet transform pipeline ready\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# VIDEO PREPROCESSING\n",
    "# ==========================================================\n",
    "\"\"\"\n",
    "- OpenCV used for video loading\n",
    "- Uniform frame sampling\n",
    "- Resize to 224Ã—224\n",
    "- ImageNet normalization (mandatory for pretrained models)\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================\n",
    "# IMAGENET PREPROCESSING TRANSFORM\n",
    "# ==========================================================\n",
    "# This transform prepares raw RGB image frames so they are\n",
    "# compatible with ImageNet-pretrained CNN backbones\n",
    "# (e.g., ResNet, EfficientNet).\n",
    "#\n",
    "# Expected input:\n",
    "#   - NumPy array or PIL Image\n",
    "#   - Shape: (H, W, C)\n",
    "#   - Value range: [0, 255]\n",
    "#\n",
    "# Output:\n",
    "#   - torch.Tensor\n",
    "#   - Shape: (C, H, W)\n",
    "#   - Normalized using ImageNet statistics\n",
    "# ==========================================================\n",
    "\n",
    "print(\"\\nğŸ¨ Initializing ImageNet preprocessing pipeline...\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Convert image to PyTorch tensor\n",
    "# ----------------------------------------------------------\n",
    "# - Converts (H, W, C) â†’ (C, H, W)\n",
    "# - Scales pixel values from [0, 255] â†’ [0.0, 1.0]\n",
    "# ----------------------------------------------------------\n",
    "print(\"ğŸ”„ Adding ToTensor():\")\n",
    "print(\"   - Converts NumPy/PIL image to torch.Tensor\")\n",
    "print(\"   - Reorders dimensions to (C, H, W)\")\n",
    "print(\"   - Scales pixel range to [0.0, 1.0]\")\n",
    "\n",
    "to_tensor_transform = transforms.ToTensor()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Normalize using ImageNet statistics\n",
    "# ----------------------------------------------------------\n",
    "# Normalization ensures that input distribution matches\n",
    "# what ImageNet-pretrained models were trained on.\n",
    "#\n",
    "# Channel order: RGB\n",
    "#\n",
    "# Formula per channel:\n",
    "#   normalized = (x - mean) / std\n",
    "# ----------------------------------------------------------\n",
    "print(\"ğŸ“ Adding Normalize():\")\n",
    "print(\"   - Mean (RGB): [0.485, 0.456, 0.406]\")\n",
    "print(\"   - Std  (RGB): [0.229, 0.224, 0.225]\")\n",
    "\n",
    "normalize_transform = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Compose transforms into a single pipeline\n",
    "# ----------------------------------------------------------\n",
    "print(\"ğŸ§© Composing preprocessing transforms\")\n",
    "\n",
    "imagenet_transform = transforms.Compose([\n",
    "    to_tensor_transform,\n",
    "    normalize_transform\n",
    "])\n",
    "\n",
    "print(\"âœ… ImageNet transform pipeline ready\")\n",
    "\n",
    "\n",
    "def load_video(video_path, num_frames=NUM_FRAMES):\n",
    "    \"\"\"\n",
    "    Load a video file, extract frames, apply spatial preprocessing,\n",
    "    perform uniform temporal sampling, and return a tensor suitable\n",
    "    for deep learning models.\n",
    "\n",
    "    Args:\n",
    "        video_path (Path or str): Path to the video file.\n",
    "        num_frames (int): Number of frames to sample uniformly.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Video tensor of shape (T, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nğŸ¬ ==================================================\")\n",
    "    print(\"ğŸ¥ Loading video\")\n",
    "    print(f\"ğŸ“ Video path      : {video_path}\")\n",
    "    print(f\"ğŸ§® Target #frames  : {num_frames}\")\n",
    "    print(\"ğŸ¬ ==================================================\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 1: Open video using OpenCV\n",
    "    # --------------------------------------------------\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "    # Sanity check: ensure video file opened correctly\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"âŒ Failed to open video file: {video_path}\")\n",
    "\n",
    "    frames = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 2: Read video frame-by-frame\n",
    "    # --------------------------------------------------\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # ret == False indicates end of video or read failure\n",
    "        if not ret:\n",
    "            print(\"â¹ï¸  End of video reached or frame read failed\")\n",
    "            break\n",
    "\n",
    "        # Convert color space from OpenCV default (BGR) to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize frame to match model input size\n",
    "        frame = cv2.resize(frame, IMG_SIZE)\n",
    "\n",
    "        frames.append(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Periodic logging for long videos\n",
    "        if frame_idx % 25 == 0:\n",
    "            print(f\"  ğŸ“¸ Frames read so far: {frame_idx}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 3: Release video capture resource\n",
    "    # --------------------------------------------------\n",
    "    cap.release()\n",
    "    print(f\"âœ… Total frames extracted: {len(frames)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 4: Uniform temporal sampling\n",
    "    # --------------------------------------------------\n",
    "    # Goal: Ensure exactly `num_frames` frames per video\n",
    "    # --------------------------------------------------\n",
    "    if len(frames) >= num_frames:\n",
    "        print(\"ğŸ“ Applying uniform temporal sampling\")\n",
    "\n",
    "        # Generate evenly spaced indices across the full video\n",
    "        idx = np.linspace(\n",
    "            0,\n",
    "            len(frames) - 1,\n",
    "            num_frames\n",
    "        ).astype(int)\n",
    "\n",
    "        print(f\"ğŸ”¢ Sampled frame indices: {idx.tolist()}\")\n",
    "\n",
    "        # Select frames at sampled indices\n",
    "        frames = [frames[i] for i in idx]\n",
    "    else:\n",
    "        print(\"âš ï¸  Video shorter than required frames\")\n",
    "        print(\"ğŸ” Padding by repeating last frame\")\n",
    "\n",
    "        # Repeat last frame until target length is reached\n",
    "        while len(frames) < num_frames:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "    print(f\"ğŸ§© Frames after temporal processing: {len(frames)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 5: Apply ImageNet normalization / transforms\n",
    "    # --------------------------------------------------\n",
    "    print(\"ğŸ¨ Applying ImageNet normalization & transforms\")\n",
    "\n",
    "    frames = [imagenet_transform(frame) for frame in frames]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 6: Stack frames into a single tensor\n",
    "    # --------------------------------------------------\n",
    "    # Final shape: (T, C, H, W)\n",
    "    video_tensor = torch.stack(frames)\n",
    "\n",
    "    print(\"ğŸ“¦ Final video tensor shape:\", video_tensor.shape)\n",
    "    print(\"ğŸ¬ ==================================================\\n\")\n",
    "\n",
    "    return video_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba39f732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------\n",
      "ğŸ“‚ Loading TRAINING data\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“„ Loading split file\n",
      "ğŸ“ Split file path: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/splits/train.txt\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“‘ Total entries found in split file: 106\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 293\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 19, 38, 58, 77, 97, 116, 136, 155, 175, 194, 214, 233, 253, 272, 292]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 174\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 34, 46, 57, 69, 80, 92, 103, 115, 126, 138, 149, 161, 173]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g19_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g17_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 153\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 81, 91, 101, 111, 121, 131, 141, 152]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g12_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 273\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 18, 36, 54, 72, 90, 108, 126, 145, 163, 181, 199, 217, 235, 253, 272]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g20_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 175\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 34, 46, 58, 69, 81, 92, 104, 116, 127, 139, 150, 162, 174]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c07.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 164\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 21, 32, 43, 54, 65, 76, 86, 97, 108, 119, 130, 141, 152, 163]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 152\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 151]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 185\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 49, 61, 73, 85, 98, 110, 122, 134, 147, 159, 171, 184]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g18_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 236\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 62, 78, 94, 109, 125, 141, 156, 172, 188, 203, 219, 235]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "  ğŸ“¸ Frames read so far: 450\n",
      "  ğŸ“¸ Frames read so far: 475\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 492\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 32, 65, 98, 130, 163, 196, 229, 261, 294, 327, 360, 392, 425, 458, 491]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g21_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g12_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 184\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 61, 73, 85, 97, 109, 122, 134, 146, 158, 170, 183]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 315\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 20, 41, 62, 83, 104, 125, 146, 167, 188, 209, 230, 251, 272, 293, 314]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g18_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 174\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 34, 46, 57, 69, 80, 92, 103, 115, 126, 138, 149, 161, 173]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g18_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 187\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 37, 49, 62, 74, 86, 99, 111, 124, 136, 148, 161, 173, 186]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g01_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 195\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181, 194]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 311\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 20, 41, 62, 82, 103, 124, 144, 165, 186, 206, 227, 248, 268, 289, 310]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g01_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 206\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 95, 109, 123, 136, 150, 164, 177, 191, 205]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g07_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 150\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139, 149]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 299\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 19, 39, 59, 79, 99, 119, 139, 158, 178, 198, 218, 238, 258, 278, 298]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g18_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 181\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 180]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g09_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g18_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 181\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 180]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g19_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g19_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g22_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 219\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 29, 43, 58, 72, 87, 101, 116, 130, 145, 159, 174, 188, 203, 218]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g22_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 339\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 22, 45, 67, 90, 112, 135, 157, 180, 202, 225, 247, 270, 292, 315, 338]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g16_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g02_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g14_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 157\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 31, 41, 52, 62, 72, 83, 93, 104, 114, 124, 135, 145, 156]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g17_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 354\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 23, 47, 70, 94, 117, 141, 164, 188, 211, 235, 258, 282, 305, 329, 353]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g04_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 213\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 42, 56, 70, 84, 98, 113, 127, 141, 155, 169, 183, 197, 212]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g15_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g15_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g07_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g06_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 235\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 46, 62, 78, 93, 109, 124, 140, 156, 171, 187, 202, 218, 234]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 207\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 96, 109, 123, 137, 151, 164, 178, 192, 206]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g08_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 201\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 40, 53, 66, 80, 93, 106, 120, 133, 146, 160, 173, 186, 200]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g10_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g13_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 239\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 126, 142, 158, 174, 190, 206, 222, 238]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g15_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 358\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 23, 47, 71, 95, 119, 142, 166, 190, 214, 238, 261, 285, 309, 333, 357]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g10_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g24_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 213\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 42, 56, 70, 84, 98, 113, 127, 141, 155, 169, 183, 197, 212]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g10_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g05_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 163\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 21, 32, 43, 54, 64, 75, 86, 97, 108, 118, 129, 140, 151, 162]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g04_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g05_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g17_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 204\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 40, 54, 67, 81, 94, 108, 121, 135, 148, 162, 175, 189, 203]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g24_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 167\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 121, 132, 143, 154, 166]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g01_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g24_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 209\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 55, 69, 83, 97, 110, 124, 138, 152, 166, 180, 194, 208]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g23_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 447\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 29, 59, 89, 118, 148, 178, 208, 237, 267, 297, 327, 356, 386, 416, 446]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g19_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g07_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 192\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 25, 38, 50, 63, 76, 89, 101, 114, 127, 140, 152, 165, 178, 191]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 324\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 21, 43, 64, 86, 107, 129, 150, 172, 193, 215, 236, 258, 279, 301, 323]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g22_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 206\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 95, 109, 123, 136, 150, 164, 177, 191, 205]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g11_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 152\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 151]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g11_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 210\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 55, 69, 83, 97, 111, 125, 139, 153, 167, 181, 195, 209]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g23_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "  ğŸ“¸ Frames read so far: 450\n",
      "  ğŸ“¸ Frames read so far: 475\n",
      "  ğŸ“¸ Frames read so far: 500\n",
      "  ğŸ“¸ Frames read so far: 525\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 535\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 35, 71, 106, 142, 178, 213, 249, 284, 320, 356, 391, 427, 462, 498, 534]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g07_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 159\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 21, 31, 42, 52, 63, 73, 84, 94, 105, 115, 126, 136, 147, 158]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g01_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g15_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g05_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g18_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g14_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g13_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 201\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 40, 53, 66, 80, 93, 106, 120, 133, 146, 160, 173, 186, 200]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g02_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 206\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 95, 109, 123, 136, 150, 164, 177, 191, 205]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g18_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g03_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g16_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 199\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 39, 52, 66, 79, 92, 105, 118, 132, 145, 158, 171, 184, 198]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g12_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 229\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 45, 60, 76, 91, 106, 121, 136, 152, 167, 182, 197, 212, 228]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g04_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g12_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g17_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g15_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g16_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g16_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g06_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 228\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 45, 60, 75, 90, 105, 121, 136, 151, 166, 181, 196, 211, 227]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g21_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g03_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g19_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 203\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 40, 53, 67, 80, 94, 107, 121, 134, 148, 161, 175, 188, 202]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g07_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 233\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 46, 61, 77, 92, 108, 123, 139, 154, 170, 185, 201, 216, 232]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g01_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g11_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g06_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g04_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g08_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c07.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g05_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g14_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g25_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ“¦ Stacking videos and labels into tensors\n",
      "âœ… Split loaded successfully !! \n",
      "ğŸ“ Videos tensor shape: torch.Size([106, 16, 3, 224, 224])\n",
      "ğŸ·ï¸  Labels tensor shape: torch.Size([106])\n",
      "ğŸ“„ ==================================================\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "ğŸ“‚ Loading TESTING data\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“„ Loading split file\n",
      "ğŸ“ Split file path: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/splits/test.txt\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“‘ Total entries found in split file: 24\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "  ğŸ“¸ Frames read so far: 450\n",
      "  ğŸ“¸ Frames read so far: 475\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 482\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448, 481]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g17_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 214\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 42, 56, 71, 85, 99, 113, 127, 142, 156, 170, 184, 198, 213]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 446\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 29, 59, 89, 118, 148, 178, 207, 237, 267, 296, 326, 356, 385, 415, 445]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g25_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 179\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 94, 106, 118, 130, 142, 154, 166, 178]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 324\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 21, 43, 64, 86, 107, 129, 150, 172, 193, 215, 236, 258, 279, 301, 323]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g21_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 182\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 181]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g07_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 300\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 19, 39, 59, 79, 99, 119, 139, 159, 179, 199, 219, 239, 259, 279, 299]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g18_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 209\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 55, 69, 83, 97, 110, 124, 138, 152, 166, 180, 194, 208]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 340\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 22, 45, 67, 90, 113, 135, 158, 180, 203, 226, 248, 271, 293, 316, 339]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g02_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 239\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 126, 142, 158, 174, 190, 206, 222, 238]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g14_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 169\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 22, 33, 44, 56, 67, 78, 89, 100, 112, 123, 134, 145, 156, 168]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g08_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g09_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 196\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 39, 52, 65, 78, 91, 104, 117, 130, 143, 156, 169, 182, 195]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g07_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g08_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 228\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 45, 60, 75, 90, 105, 121, 136, 151, 166, 181, 196, 211, 227]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 237\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 62, 78, 94, 110, 125, 141, 157, 173, 188, 204, 220, 236]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g14_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 216\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 43, 57, 71, 86, 100, 114, 129, 143, 157, 172, 186, 200, 215]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g11_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 239\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 126, 142, 158, 174, 190, 206, 222, 238]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g17_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g24_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g17_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ“¦ Stacking videos and labels into tensors\n",
      "âœ… Split loaded successfully !! \n",
      "ğŸ“ Videos tensor shape: torch.Size([24, 16, 3, 224, 224])\n",
      "ğŸ·ï¸  Labels tensor shape: torch.Size([24])\n",
      "ğŸ“„ ==================================================\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "âœ… Dataset summary\n",
      "-----------------------------------------------\n",
      "\n",
      "ğŸ“ Training videos : 106\n",
      "ğŸ§ª Testing videos  : 24\n",
      "ğŸ·ï¸  Total classes  : 3\n",
      "ğŸ“ Train tensor    : torch.Size([106, 16, 3, 224, 224])  (N, T, C, H, W)\n",
      "ğŸ“ Test tensor     : torch.Size([24, 16, 3, 224, 224])   (N, T, C, H, W)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# LOAD DATA USING OFFICIAL TRAIN / TEST SPLITS\n",
    "# ==========================================================\n",
    "# This section loads videos and labels using pre-defined\n",
    "# split files (e.g., train.txt, test.txt).\n",
    "#\n",
    "# Each split file is expected to contain relative paths\n",
    "# to video files, one per line, such as:\n",
    "#\n",
    "#   class_0/video_001.avi\n",
    "#   class_1/video_023.avi\n",
    "#\n",
    "# The parent folder name (class_*) is used as the label.\n",
    "# ==========================================================\n",
    "\n",
    "def load_split(split_file):\n",
    "    \"\"\"\n",
    "    Load videos and labels from a split file.\n",
    "\n",
    "    Args:\n",
    "        split_file (Path): Path to the split text file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]:\n",
    "            - videos: Tensor of shape (N, T, C, H, W)\n",
    "            - labels: Tensor of shape (N,)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nğŸ“„ ==================================================\")\n",
    "    print(\"ğŸ“„ Loading split file\")\n",
    "    print(f\"ğŸ“ Split file path: {split_file}\")\n",
    "    print(\"ğŸ“„ ==================================================\")\n",
    "\n",
    "    videos = []  # Will store per-video tensors\n",
    "    labels = []  # Will store integer class labels\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 1: Read split file\n",
    "    # --------------------------------------------------\n",
    "    with open(split_file, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    print(f\"ğŸ“‘ Total entries found in split file: {len(lines)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 2: Process each video entry\n",
    "    # --------------------------------------------------\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Construct absolute video path\n",
    "        video_path = DATASET_ROOT / line\n",
    "\n",
    "        # Extract class name from path\n",
    "        # Example: \"class_2/video_003.avi\" â†’ \"class_2\"\n",
    "        class_name = line.split(\"/\")[0]\n",
    "\n",
    "        # Map class name to integer label\n",
    "        label = CLASS_TO_IDX[class_name]\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ¬ ----------------------------------------------\")\n",
    "        print(f\"ğŸ¥ Processing video {idx + 1}/{len(lines)}\")\n",
    "        print(f\"ğŸ“ Relative path : {line}\")\n",
    "        print(f\"ğŸ“ Absolute path : {video_path}\")\n",
    "        print(f\"ğŸ·ï¸  Class name   : {class_name}\")\n",
    "        print(f\"ğŸ”¢ Class index  : {label}\")\n",
    "        print(\"ğŸ¬ ----------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Load and preprocess video (T, C, H, W)\n",
    "        video_tensor = load_video(video_path)\n",
    "\n",
    "        # Append video tensor and label\n",
    "        videos.append(video_tensor)\n",
    "        labels.append(label)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 3: Stack videos and labels into tensors\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nğŸ“¦ Stacking videos and labels into tensors\")\n",
    "\n",
    "    videos_tensor = torch.stack(videos)        # Shape: (N, T, C, H, W)\n",
    "    labels_tensor = torch.tensor(labels)       # Shape: (N,)\n",
    "\n",
    "    print(\"âœ… Split loaded successfully !! \")\n",
    "    print(f\"ğŸ“ Videos tensor shape: {videos_tensor.shape}\")\n",
    "    print(f\"ğŸ·ï¸  Labels tensor shape: {labels_tensor.shape}\")\n",
    "    print(\"ğŸ“„ ==================================================\\n\")\n",
    "\n",
    "    return videos_tensor, labels_tensor\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD TRAINING DATA\n",
    "# ==========================================================\n",
    "print(\"\\n-----------------------------------------------\")\n",
    "print(\"ğŸ“‚ Loading TRAINING data\")\n",
    "print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "X_train, y_train = load_split(SPLITS_DIR / \"train.txt\")\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD TESTING DATA\n",
    "# ==========================================================\n",
    "print(\"\\n-----------------------------------------------\")\n",
    "print(\"ğŸ“‚ Loading TESTING data\")\n",
    "print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "X_test, y_test = load_split(SPLITS_DIR / \"test.txt\")\n",
    "\n",
    "# ==========================================================\n",
    "# DATASET SUMMARY\n",
    "# ==========================================================\n",
    "print(\"\\n-----------------------------------------------\")\n",
    "print(\"âœ… Dataset summary\")\n",
    "print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "print(f\"ğŸ“ Training videos : {len(X_train)}\")\n",
    "print(f\"ğŸ§ª Testing videos  : {len(X_test)}\")\n",
    "print(f\"ğŸ·ï¸  Total classes  : {NUM_CLASSES}\")\n",
    "print(f\"ğŸ“ Train tensor    : {X_train.shape}  (N, T, C, H, W)\")\n",
    "print(f\"ğŸ“ Test tensor     : {X_test.shape}   (N, T, C, H, W)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ca61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# VIDEO DATASET WITH OPTIONAL DATA AUGMENTATION\n",
    "# ==========================================================\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for video classification.\n",
    "\n",
    "    Each sample consists of:\n",
    "    - A video: sequence of frames (Tensor)\n",
    "    - A label: class index or class name\n",
    "\n",
    "    Data augmentation:\n",
    "    - Random horizontal flip is applied\n",
    "      ONLY when train=True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: List[torch.Tensor],\n",
    "        y: List[int],\n",
    "        train: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            X (List[Tensor]):\n",
    "                List of videos.\n",
    "                Each video is a Tensor of shape:\n",
    "                (num_frames, channels, height, width)\n",
    "\n",
    "            y (List[int]):\n",
    "                Corresponding labels for each video.\n",
    "\n",
    "            train (bool):\n",
    "                If True:\n",
    "                    - Apply data augmentation (horizontal flip)\n",
    "                If False:\n",
    "                    - No augmentation (used for validation/testing)\n",
    "        \"\"\"\n",
    "        # Store videos\n",
    "        self.X = X\n",
    "\n",
    "        # Store labels\n",
    "        self.y = y\n",
    "\n",
    "        # Flag to control augmentation behavior\n",
    "        self.train = train\n",
    "\n",
    "        # Define spatial augmentation:\n",
    "        # Randomly flips an image horizontally with 50% probability\n",
    "        #\n",
    "        # IMPORTANT:\n",
    "        # - This does NOT add new pixels\n",
    "        # - It only rearranges existing pixels\n",
    "        self.flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "\n",
    "        This method is required by PyTorch's Dataset class\n",
    "        so that DataLoader knows how many samples exist.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Retrieve one sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int):\n",
    "                Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, int]:\n",
    "                - video: Tensor of shape\n",
    "                  (num_frames, channels, height, width)\n",
    "                - label: corresponding class label\n",
    "        \"\"\"\n",
    "        # Fetch the video at the given index\n",
    "        video = self.X[idx]\n",
    "\n",
    "        # Apply data augmentation ONLY during training\n",
    "        if self.train:\n",
    "            # Apply horizontal flip independently to each frame\n",
    "            #\n",
    "            # Why per-frame?\n",
    "            # - Each frame is treated as an image\n",
    "            # - Maintains temporal order\n",
    "            # - Simple and effective spatial augmentation\n",
    "            #\n",
    "            # torch.stack is used to reconstruct the video\n",
    "            # back into a single Tensor\n",
    "            video = torch.stack([self.flip(frame) for frame in video])\n",
    "\n",
    "        # Return video and its label\n",
    "        return video, self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaf95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data loaders initialized\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# DATALOADERS\n",
    "# ==========================================================\n",
    "train_loader = DataLoader(\n",
    "    VideoDataset(X_train, y_train, train=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    VideoDataset(X_test, y_test, train=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Data loaders initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af423aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# TRAINING & EVALUATION FUNCTION\n",
    "# ==========================================================\n",
    "def train_and_evaluate(model, model_name):\n",
    "    \"\"\"\n",
    "    Train a deep learning model and evaluate its performance.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Creates result directories for storing outputs\n",
    "    2. Trains the model for a fixed number of epochs\n",
    "    3. Records training time\n",
    "    4. Evaluates the model on a test set\n",
    "    5. Computes classification metrics\n",
    "    6. Saves training loss plots and confusion matrices\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module):\n",
    "            The neural network model to train and evaluate\n",
    "        model_name (str):\n",
    "            Descriptive name of the model for logging and file naming\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            Dictionary containing performance metrics and confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Create directory structure for storing results\n",
    "    # ------------------------------------------------------\n",
    "    # Base directory where all experiment artifacts are stored\n",
    "    base_dir = Path(\"results\")\n",
    "\n",
    "    # Subdirectory for confusion matrix visualizations\n",
    "    cm_dir = base_dir / \"confusion_matrices\"\n",
    "\n",
    "    # Subdirectory for training curves and performance plots\n",
    "    perf_dir = base_dir / \"performance_plots\"\n",
    "\n",
    "    # Reserved directory for feature-level visualizations\n",
    "    feat_dir = base_dir / \"feature_visualizations\"\n",
    "\n",
    "    # Create directories if they do not already exist\n",
    "    cm_dir.mkdir(parents=True, exist_ok=True)\n",
    "    perf_dir.mkdir(parents=True, exist_ok=True)\n",
    "    feat_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Model setup and initialization\n",
    "    # ------------------------------------------------------\n",
    "    print(f\"\\nğŸš€ Training {model_name}\")\n",
    "\n",
    "    # Move the model to the appropriate device (CPU / GPU)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Optimizer and loss function definition\n",
    "    # ------------------------------------------------------\n",
    "    # Adam optimizer is chosen for faster convergence and stability\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=1e-4  # L2 regularization to reduce overfitting\n",
    "    )\n",
    "\n",
    "    # Cross-entropy loss is standard for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Training phase\n",
    "    # ------------------------------------------------------\n",
    "    # Switch model to training mode\n",
    "    # Enables dropout and batch normalization updates\n",
    "    model.train()\n",
    "\n",
    "    # Record the start time of training\n",
    "    start_train = time.time()\n",
    "\n",
    "    # List to store average loss per epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Iterate over training epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # Accumulator for total loss in the epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over mini-batches from the training loader\n",
    "        for videos, labels in train_loader:\n",
    "\n",
    "            # Move input data and labels to device\n",
    "            videos = videos.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # Clear gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute model outputs\n",
    "            outputs = model(videos)\n",
    "\n",
    "            # Compute loss between predictions and ground truth\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        # Log training progress\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{EPOCHS}] \"\n",
    "            f\"- Avg Loss: {avg_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Compute total training time\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Save training loss curve\n",
    "    # ------------------------------------------------------\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_losses, marker=\"o\")\n",
    "    plt.title(f\"{model_name} - Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Generate file-safe model name\n",
    "    loss_plot_path = perf_dir / f\"{model_name.replace(' ', '_')}_loss.png\"\n",
    "\n",
    "    # Save plot to disk\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Evaluation phase (Inference)\n",
    "    # ------------------------------------------------------\n",
    "    # Switch model to evaluation mode\n",
    "    # Disables dropout and freezes batch normalization\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store predictions and ground-truth labels\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    # Record inference start time\n",
    "    start_inf = time.time()\n",
    "\n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in test_loader:\n",
    "\n",
    "            # Move input videos to device\n",
    "            videos = videos.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(videos)\n",
    "\n",
    "            # Convert logits to predicted class indices\n",
    "            batch_preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            preds.extend(batch_preds.cpu().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "\n",
    "    # Compute average inference time per video\n",
    "    inf_time = (time.time() - start_inf) / len(targets)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Metric computation\n",
    "    # ------------------------------------------------------\n",
    "    metrics = {\n",
    "        # Overall classification accuracy\n",
    "        \"accuracy\": accuracy_score(targets, preds),\n",
    "\n",
    "        # Precision averaged equally across classes\n",
    "        \"precision\": precision_score(targets, preds, average=\"macro\"),\n",
    "\n",
    "        # Recall averaged equally across classes\n",
    "        \"recall\": recall_score(targets, preds, average=\"macro\"),\n",
    "\n",
    "        # Harmonic mean of precision and recall\n",
    "        \"f1\": f1_score(targets, preds, average=\"macro\"),\n",
    "\n",
    "        # Total training duration in seconds\n",
    "        \"train_time\": train_time,\n",
    "\n",
    "        # Average inference time per sample\n",
    "        \"inf_time\": inf_time,\n",
    "\n",
    "        # Confusion matrix for class-wise error analysis\n",
    "        \"cm\": confusion_matrix(targets, preds)\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Print evaluation results\n",
    "    # ------------------------------------------------------\n",
    "    print(f\"\\nğŸ“Š {model_name} Performance\")\n",
    "    for key, value in metrics.items():\n",
    "        if key != \"cm\":\n",
    "            print(\n",
    "                f\"{key.replace('_', ' ').title():<20}: \"\n",
    "                f\"{value:.4f}\"\n",
    "            )\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Save confusion matrix visualization\n",
    "    # ------------------------------------------------------\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        metrics[\"cm\"],\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\"\n",
    "    )\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Generate file-safe path for confusion matrix\n",
    "    cm_path = cm_dir / f\"{model_name.replace(' ', '_')}_cm.png\"\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe317536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training 2D CNN + Temporal Pooling\n",
      "Epoch [1/5] - Avg Loss: 1.1055\n",
      "Epoch [2/5] - Avg Loss: 0.7534\n",
      "Epoch [3/5] - Avg Loss: 0.5529\n",
      "Epoch [4/5] - Avg Loss: 0.4728\n",
      "Epoch [5/5] - Avg Loss: 0.4671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R2Plus1D_18_Weights.KINETICS400_V1`. You can also use `weights=R2Plus1D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 2D CNN + Temporal Pooling Performance\n",
      "Accuracy            : 0.4583\n",
      "Precision           : 0.4697\n",
      "Recall              : 0.4074\n",
      "F1                  : 0.3148\n",
      "Train Time          : 15.3255\n",
      "Inf Time            : 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120M/120M [00:01<00:00, 110MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training 3D CNN (R(2+1)D)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Avg Loss: 0.7713\n",
      "Epoch [2/5] - Avg Loss: 0.3521\n",
      "Epoch [3/5] - Avg Loss: 0.2016\n",
      "Epoch [4/5] - Avg Loss: 0.0870\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# RUN EXPERIMENTS\n",
    "# ==========================================================\n",
    "metrics_2d = train_and_evaluate(\n",
    "    CNN2DTemporal(NUM_CLASSES),\n",
    "    \"2D CNN + Temporal Pooling\"\n",
    ")\n",
    "\n",
    "metrics_3d = train_and_evaluate(\n",
    "    CNN3D(NUM_CLASSES),\n",
    "    \"3D CNN (R(2+1)D)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb1a942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“ˆ DETAILED MODEL COMPARISON REPORT\n",
      "============================================================\n",
      "\n",
      "ğŸ”¹ Accuracy Comparison\n",
      "2D CNN Accuracy : 1.0000\n",
      "3D CNN Accuracy : 1.0000\n",
      "\n",
      "ğŸ”¹ Computational Efficiency\n",
      "2D CNN Training Time (s)        : 14.81\n",
      "3D CNN Training Time (s)        : 65.04\n",
      "2D CNN Inference Time / Video(s): 0.0092\n",
      "3D CNN Inference Time / Video(s): 0.0192\n",
      "\n",
      "ğŸ”¹ Qualitative Analysis\n",
      "âœ” The 2D CNN achieves competitive accuracy with significantly lower computational cost, making it suitable for real-time applications.\n",
      "\n",
      "ğŸ”¹ Final Conclusion:\n",
      "2D CNNs provide a strong baseline with efficient inference, while 3D CNNs offer improved performance at the cost of higher computation.\n",
      "\n",
      "âœ… Experiment completed successfully\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# DETAILED COMPARISON REPORT\n",
    "# ==========================================================\n",
    "\n",
    "def compare_models(metrics_2d, metrics_3d):\n",
    "    \"\"\"\n",
    "    Generate comparison charts and a detailed report\n",
    "    for 2D CNN vs 3D CNN models.\n",
    "\n",
    "    Args:\n",
    "        metrics_2d (dict): Metrics for 2D CNN\n",
    "        metrics_3d (dict): Metrics for 3D CNN\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Sanity checks\n",
    "    # ------------------------------------------------------\n",
    "    required_keys = {\"accuracy\", \"train_time\", \"inf_time\"}\n",
    "\n",
    "    if not required_keys.issubset(metrics_2d):\n",
    "        raise ValueError(\"metrics_2d missing required keys\")\n",
    "\n",
    "    if not required_keys.issubset(metrics_3d):\n",
    "        raise ValueError(\"metrics_3d missing required keys\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Extract metrics\n",
    "    # ------------------------------------------------------\n",
    "    models = [\"2D CNN\", \"3D CNN\"]\n",
    "\n",
    "    accuracy = [\n",
    "        metrics_2d[\"accuracy\"],\n",
    "        metrics_3d[\"accuracy\"]\n",
    "    ]\n",
    "\n",
    "    train_time = [\n",
    "        metrics_2d[\"train_time\"],\n",
    "        metrics_3d[\"train_time\"]\n",
    "    ]\n",
    "\n",
    "    inf_time = [\n",
    "        metrics_2d[\"inf_time\"],\n",
    "        metrics_3d[\"inf_time\"]\n",
    "    ]\n",
    "\n",
    "    # ======================================================\n",
    "    # CHART 1: Accuracy Comparison\n",
    "    # ======================================================\n",
    "    plt.figure()\n",
    "    plt.bar(models, accuracy)\n",
    "    plt.title(\"Model Accuracy Comparison\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    # ======================================================\n",
    "    # CHART 2: Training Time Comparison\n",
    "    # ======================================================\n",
    "    plt.figure()\n",
    "    plt.bar(models, train_time)\n",
    "    plt.title(\"Training Time Comparison\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ======================================================\n",
    "    # CHART 3: Inference Time Comparison\n",
    "    # ======================================================\n",
    "    plt.figure()\n",
    "    plt.bar(models, inf_time)\n",
    "    plt.title(\"Inference Time per Video Comparison\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ======================================================\n",
    "    # TEXTUAL COMPARISON REPORT\n",
    "    # ======================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“ˆ DETAILED MODEL COMPARISON REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Accuracy\n",
    "    print(\"\\nğŸ”¹ Accuracy Comparison\")\n",
    "    print(f\"2D CNN Accuracy : {metrics_2d['accuracy']:.4f}\")\n",
    "    print(f\"3D CNN Accuracy : {metrics_3d['accuracy']:.4f}\")\n",
    "\n",
    "    # Efficiency\n",
    "    print(\"\\nğŸ”¹ Computational Efficiency\")\n",
    "    print(f\"2D CNN Training Time (s)        : {metrics_2d['train_time']:.2f}\")\n",
    "    print(f\"3D CNN Training Time (s)        : {metrics_3d['train_time']:.2f}\")\n",
    "    print(f\"2D CNN Inference Time / Video(s): {metrics_2d['inf_time']:.4f}\")\n",
    "    print(f\"3D CNN Inference Time / Video(s): {metrics_3d['inf_time']:.4f}\")\n",
    "\n",
    "    # Qualitative analysis\n",
    "    print(\"\\nğŸ”¹ Qualitative Analysis\")\n",
    "    if metrics_3d[\"accuracy\"] > metrics_2d[\"accuracy\"]:\n",
    "        print(\n",
    "            \"âœ” The 3D CNN outperforms the 2D CNN by explicitly modeling \"\n",
    "            \"spatiotemporal patterns, making it more suitable for complex actions.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"âœ” The 2D CNN achieves competitive accuracy with significantly lower \"\n",
    "            \"computational cost, making it suitable for real-time applications.\"\n",
    "        )\n",
    "\n",
    "    # Final conclusion\n",
    "    print(\n",
    "        \"\\nğŸ”¹ Final Conclusion:\\n\"\n",
    "        \"2D CNNs provide a strong baseline with efficient inference, while \"\n",
    "        \"3D CNNs offer improved performance at the cost of higher computation.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ… Experiment completed successfully\")\n",
    "\n",
    "compare_models(metrics_2d, metrics_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bd8d3-145d-4563-add2-edb0e37b0bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
