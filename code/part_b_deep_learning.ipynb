{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa53da25",
   "metadata": {},
   "source": [
    "PART B: DEEP LEARNING VIDEO CLASSIFICATION (REAL DATA)\n",
    "\n",
    "This script implements:\n",
    "1. 2D CNN (ResNet-18) + Temporal Pooling\n",
    "2. 3D CNN (R(2+1)D-18)\n",
    "\n",
    "Dataset:\n",
    "- UCF-style directory\n",
    "- Predefined train/test splits\n",
    "\n",
    "Evaluation:\n",
    "- Accuracy\n",
    "- Precision (macro)\n",
    "- Recall (macro)\n",
    "- F1-score (macro)\n",
    "- Confusion Matrix\n",
    "- Training time\n",
    "- Inference time per video\n",
    "\n",
    "Author: 2024ab05275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7155a69f-1727-4343-8b54-45bfac7cbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.11.3)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.3.0+cu121)\n",
      "Requirement already satisfied: torchvision>=0.15 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.18.0+cu121)\n",
      "Collecting opencv-python-headless (from -r requirements.txt (line 10))\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.22.0)\n",
      "Requirement already satisfied: matplotlib>=3.7 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (3.8.4)\n",
      "Requirement already satisfied: seaborn>=0.13 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.66 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision>=0.15->-r requirements.txt (line 7)) (10.4.0)\n",
      "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.3->-r requirements.txt (line 11)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.3->-r requirements.txt (line 11)) (3.5.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (2024.7.2)\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (24.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.11/site-packages (from scikit-image->-r requirements.txt (line 12)) (0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7->-r requirements.txt (line 15)) (2.9.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn>=0.13->-r requirements.txt (line 16)) (2.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn>=0.13->-r requirements.txt (line 16)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn>=0.13->-r requirements.txt (line 16)) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.7->-r requirements.txt (line 15)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf86cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# IMPORTS\n",
    "# ==========================================================\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Standard Library Imports\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import os\n",
    "# File-system operations such as path checks and environment handling\n",
    "\n",
    "import time\n",
    "# Execution time measurement and performance benchmarking\n",
    "\n",
    "from pathlib import Path\n",
    "# Object-oriented and platform-independent file path management\n",
    "\n",
    "from typing import List, Tuple\n",
    "# Type annotations for improved readability and static analysis\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Scientific Computing & Computer Vision Libraries\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "# Numerical computation, array manipulation, and statistical operations\n",
    "\n",
    "import cv2\n",
    "# Video decoding, frame preprocessing, color-space conversion, and filtering\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PyTorch Core Framework\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "# Tensor computation, GPU acceleration, and automatic differentiation\n",
    "\n",
    "import torch.nn as nn\n",
    "# Neural network layer definitions and model construction primitives\n",
    "\n",
    "import torch.optim as optim\n",
    "# Optimization algorithms for training neural networks\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Dataset abstraction and efficient batch-wise data loading\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Torchvision Utilities\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "from torchvision import models, transforms\n",
    "# Pretrained CNN backbones and image preprocessing pipelines\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluation Metrics (Scikit-learn)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "# Classification performance metrics and confusion matrix analysis\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Visualization Libraries\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting utilities for training curves and comparative analysis\n",
    "\n",
    "import seaborn as sns\n",
    "# High-level statistical visualizations and heatmaps\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Project-Specific Model Architectures\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "from models import CNN2DTemporal, CNN3D\n",
    "# Custom deep learning models for video classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9008d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "âœ… Project root: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification\n",
      "âœ… Dataset root: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset\n",
      "âœ… Local Weight Path : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/model/resnet18-f37072fd.pth\n",
      "----------------------------------------------------------\n",
      "ğŸš€ Running on device: cuda\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================\n",
    "# Current file is inside /code\n",
    "CODE_DIR = Path.cwd()\n",
    "\n",
    "# Project root is one level above\n",
    "PROJECT_ROOT = CODE_DIR.parent\n",
    "\n",
    "DATASET_ROOT = PROJECT_ROOT / \"dataset\"\n",
    "SPLITS_DIR = DATASET_ROOT / \"splits\"\n",
    "\n",
    "LOCAL_WEIGHTS = os.path.join(PROJECT_ROOT, \"model\", \"resnet18-f37072fd.pth\")\n",
    "\n",
    "# Safety checks (VERY IMPORTANT)\n",
    "assert DATASET_ROOT.exists(), f\"Dataset not found at {DATASET_ROOT}\"\n",
    "assert SPLITS_DIR.exists(), f\"Splits folder not found at {SPLITS_DIR}\"\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Dataset root: {DATASET_ROOT}\")\n",
    "print(f\"âœ… Local Weight Path : {LOCAL_WEIGHTS}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "NUM_FRAMES = 16                 # Frames sampled per video\n",
    "IMG_SIZE = (224, 224)           # Required for pretrained CNNs\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸš€ Running on device: {DEVICE}\")\n",
    "print(\"----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73aa49f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Scanning dataset root for class folders.....\n",
      "* Dataset root path: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset\n",
      "----------------------------------------------------------\n",
      "* No of Class Folders Found : 3\n",
      "* Detected class folders (sorted):\n",
      "  - class_1_Basketball\n",
      "  - class_2_Biking\n",
      "  - class_3_WalkingWithDog\n",
      "----------------------------------------------------------\n",
      "* Final class-to-index mapping:\n",
      "  - class_1_Basketball â†’ 0\n",
      "  - class_2_Biking â†’ 1\n",
      "  - class_3_WalkingWithDog â†’ 2\n",
      "----------------------------------------------------------\n",
      "* Total number of classes: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CLASS MAPPING (Derived from dataset folder names)\n",
    "# ==========================================================\n",
    "# This section automatically discovers class labels from the\n",
    "# dataset directory structure.\n",
    "#\n",
    "# Expected directory format:\n",
    "# DATASET_ROOT/\n",
    "# â”œâ”€â”€ class_0/\n",
    "# â”œâ”€â”€ class_1/\n",
    "# â”œâ”€â”€ class_2/\n",
    "# â””â”€â”€ ...\n",
    "#\n",
    "# Each \"class_*\" folder represents one target class.\n",
    "# ==========================================================\n",
    "\n",
    "print(\"* Scanning dataset root for class folders.....\")\n",
    "print(f\"* Dataset root path: {DATASET_ROOT}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Discover class directory names\n",
    "# ----------------------------------------------------------\n",
    "# - Iterate over all items inside DATASET_ROOT\n",
    "# - Keep only directories\n",
    "# - Keep only directory names that start with \"class_\"\n",
    "# - Sort them to ensure consistent class index assignment\n",
    "# ----------------------------------------------------------\n",
    "CLASS_NAMES = sorted([\n",
    "    d.name                      # Folder name (e.g., \"class_1\")\n",
    "    for d in DATASET_ROOT.iterdir()\n",
    "    if d.is_dir()                # Ensure it is a directory\n",
    "    and d.name.startswith(\"class_\")  # Enforce naming convention\n",
    "])\n",
    "\n",
    "print(f\"* No of Class Folders Found : {len(CLASS_NAMES)}\")\n",
    "print(\"* Detected class folders (sorted):\")\n",
    "for cls in CLASS_NAMES:\n",
    "    print(f\"  - {cls}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Create class-to-index mapping\n",
    "# ----------------------------------------------------------\n",
    "# Assign a unique integer label to each class name.\n",
    "# The index order is determined by the sorted CLASS_NAMES list.\n",
    "#\n",
    "# Example:\n",
    "#   class_0 -> 0\n",
    "#   class_1 -> 1\n",
    "# ----------------------------------------------------------\n",
    "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Count total number of classes\n",
    "# ----------------------------------------------------------\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Log final class mapping\n",
    "# ----------------------------------------------------------\n",
    "print(\"* Final class-to-index mapping:\")\n",
    "for class_name, class_idx in CLASS_TO_IDX.items():\n",
    "    print(f\"  - {class_name} â†’ {class_idx}\")\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"* Total number of classes: {NUM_CLASSES}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a358024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¨ Initializing ImageNet preprocessing pipeline...\n",
      "ğŸ”„ Adding ToTensor():\n",
      "   - Converts NumPy/PIL image to torch.Tensor\n",
      "   - Reorders dimensions to (C, H, W)\n",
      "   - Scales pixel range to [0.0, 1.0]\n",
      "ğŸ“ Adding Normalize():\n",
      "   - Mean (RGB): [0.485, 0.456, 0.406]\n",
      "   - Std  (RGB): [0.229, 0.224, 0.225]\n",
      "ğŸ§© Composing preprocessing transforms\n",
      "âœ… ImageNet transform pipeline ready\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# VIDEO PREPROCESSING\n",
    "# ==========================================================\n",
    "\"\"\"\n",
    "- OpenCV used for video loading\n",
    "- Uniform frame sampling\n",
    "- Resize to 224Ã—224\n",
    "- ImageNet normalization (mandatory for pretrained models)\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================\n",
    "# IMAGENET PREPROCESSING TRANSFORM\n",
    "# ==========================================================\n",
    "# This transform prepares raw RGB image frames so they are\n",
    "# compatible with ImageNet-pretrained CNN backbones\n",
    "# (e.g., ResNet, EfficientNet).\n",
    "#\n",
    "# Expected input:\n",
    "#   - NumPy array or PIL Image\n",
    "#   - Shape: (H, W, C)\n",
    "#   - Value range: [0, 255]\n",
    "#\n",
    "# Output:\n",
    "#   - torch.Tensor\n",
    "#   - Shape: (C, H, W)\n",
    "#   - Normalized using ImageNet statistics\n",
    "# ==========================================================\n",
    "\n",
    "print(\"\\nğŸ¨ Initializing ImageNet preprocessing pipeline...\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Convert image to PyTorch tensor\n",
    "# ----------------------------------------------------------\n",
    "# - Converts (H, W, C) â†’ (C, H, W)\n",
    "# - Scales pixel values from [0, 255] â†’ [0.0, 1.0]\n",
    "# ----------------------------------------------------------\n",
    "print(\"ğŸ”„ Adding ToTensor():\")\n",
    "print(\"   - Converts NumPy/PIL image to torch.Tensor\")\n",
    "print(\"   - Reorders dimensions to (C, H, W)\")\n",
    "print(\"   - Scales pixel range to [0.0, 1.0]\")\n",
    "\n",
    "to_tensor_transform = transforms.ToTensor()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Normalize using ImageNet statistics\n",
    "# ----------------------------------------------------------\n",
    "# Normalization ensures that input distribution matches\n",
    "# what ImageNet-pretrained models were trained on.\n",
    "#\n",
    "# Channel order: RGB\n",
    "#\n",
    "# Formula per channel:\n",
    "#   normalized = (x - mean) / std\n",
    "# ----------------------------------------------------------\n",
    "print(\"ğŸ“ Adding Normalize():\")\n",
    "print(\"   - Mean (RGB): [0.485, 0.456, 0.406]\")\n",
    "print(\"   - Std  (RGB): [0.229, 0.224, 0.225]\")\n",
    "\n",
    "normalize_transform = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Compose transforms into a single pipeline\n",
    "# ----------------------------------------------------------\n",
    "print(\"ğŸ§© Composing preprocessing transforms\")\n",
    "\n",
    "imagenet_transform = transforms.Compose([\n",
    "    to_tensor_transform,\n",
    "    normalize_transform\n",
    "])\n",
    "\n",
    "print(\"âœ… ImageNet transform pipeline ready\")\n",
    "\n",
    "\n",
    "def load_video(video_path, num_frames=NUM_FRAMES):\n",
    "    \"\"\"\n",
    "    Load a video file, extract frames, apply spatial preprocessing,\n",
    "    perform uniform temporal sampling, and return a tensor suitable\n",
    "    for deep learning models.\n",
    "\n",
    "    Args:\n",
    "        video_path (Path or str): Path to the video file.\n",
    "        num_frames (int): Number of frames to sample uniformly.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Video tensor of shape (T, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nğŸ¬ ==================================================\")\n",
    "    print(\"ğŸ¥ Loading video\")\n",
    "    print(f\"ğŸ“ Video path      : {video_path}\")\n",
    "    print(f\"ğŸ§® Target #frames  : {num_frames}\")\n",
    "    print(\"ğŸ¬ ==================================================\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 1: Open video using OpenCV\n",
    "    # --------------------------------------------------\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "    # Sanity check: ensure video file opened correctly\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"âŒ Failed to open video file: {video_path}\")\n",
    "\n",
    "    frames = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 2: Read video frame-by-frame\n",
    "    # --------------------------------------------------\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # ret == False indicates end of video or read failure\n",
    "        if not ret:\n",
    "            print(\"â¹ï¸  End of video reached or frame read failed\")\n",
    "            break\n",
    "\n",
    "        # Convert color space from OpenCV default (BGR) to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize frame to match model input size\n",
    "        frame = cv2.resize(frame, IMG_SIZE)\n",
    "\n",
    "        frames.append(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Periodic logging for long videos\n",
    "        if frame_idx % 25 == 0:\n",
    "            print(f\"  ğŸ“¸ Frames read so far: {frame_idx}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 3: Release video capture resource\n",
    "    # --------------------------------------------------\n",
    "    cap.release()\n",
    "    print(f\"âœ… Total frames extracted: {len(frames)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 4: Uniform temporal sampling\n",
    "    # --------------------------------------------------\n",
    "    # Goal: Ensure exactly `num_frames` frames per video\n",
    "    # --------------------------------------------------\n",
    "    if len(frames) >= num_frames:\n",
    "        print(\"ğŸ“ Applying uniform temporal sampling\")\n",
    "\n",
    "        # Generate evenly spaced indices across the full video\n",
    "        idx = np.linspace(\n",
    "            0,\n",
    "            len(frames) - 1,\n",
    "            num_frames\n",
    "        ).astype(int)\n",
    "\n",
    "        print(f\"ğŸ”¢ Sampled frame indices: {idx.tolist()}\")\n",
    "\n",
    "        # Select frames at sampled indices\n",
    "        frames = [frames[i] for i in idx]\n",
    "    else:\n",
    "        print(\"âš ï¸  Video shorter than required frames\")\n",
    "        print(\"ğŸ” Padding by repeating last frame\")\n",
    "\n",
    "        # Repeat last frame until target length is reached\n",
    "        while len(frames) < num_frames:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "    print(f\"ğŸ§© Frames after temporal processing: {len(frames)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 5: Apply ImageNet normalization / transforms\n",
    "    # --------------------------------------------------\n",
    "    print(\"ğŸ¨ Applying ImageNet normalization & transforms\")\n",
    "\n",
    "    frames = [imagenet_transform(frame) for frame in frames]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 6: Stack frames into a single tensor\n",
    "    # --------------------------------------------------\n",
    "    # Final shape: (T, C, H, W)\n",
    "    video_tensor = torch.stack(frames)\n",
    "\n",
    "    print(\"ğŸ“¦ Final video tensor shape:\", video_tensor.shape)\n",
    "    print(\"ğŸ¬ ==================================================\\n\")\n",
    "\n",
    "    return video_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba39f732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------\n",
      "ğŸ“‚ Loading TRAINING data\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“„ Loading split file\n",
      "ğŸ“ Split file path: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/splits/train.txt\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“‘ Total entries found in split file: 106\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 293\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 19, 38, 58, 77, 97, 116, 136, 155, 175, 194, 214, 233, 253, 272, 292]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 174\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 34, 46, 57, 69, 80, 92, 103, 115, 126, 138, 149, 161, 173]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g19_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g17_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 153\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 81, 91, 101, 111, 121, 131, 141, 152]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g12_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 273\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 18, 36, 54, 72, 90, 108, 126, 145, 163, 181, 199, 217, 235, 253, 272]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g20_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 175\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 34, 46, 58, 69, 81, 92, 104, 116, 127, 139, 150, 162, 174]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c07.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 164\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 21, 32, 43, 54, 65, 76, 86, 97, 108, 119, 130, 141, 152, 163]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 152\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 151]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 185\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 49, 61, 73, 85, 98, 110, 122, 134, 147, 159, 171, 184]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g18_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 236\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 62, 78, 94, 109, 125, 141, 156, 172, 188, 203, 219, 235]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "  ğŸ“¸ Frames read so far: 450\n",
      "  ğŸ“¸ Frames read so far: 475\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 492\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 32, 65, 98, 130, 163, 196, 229, 261, 294, 327, 360, 392, 425, 458, 491]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g21_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g12_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 184\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 61, 73, 85, 97, 109, 122, 134, 146, 158, 170, 183]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 315\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 20, 41, 62, 83, 104, 125, 146, 167, 188, 209, 230, 251, 272, 293, 314]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g18_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 174\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 34, 46, 57, 69, 80, 92, 103, 115, 126, 138, 149, 161, 173]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g18_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 187\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 37, 49, 62, 74, 86, 99, 111, 124, 136, 148, 161, 173, 186]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g01_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g15_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 195\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181, 194]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 311\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 20, 41, 62, 82, 103, 124, 144, 165, 186, 206, 227, 248, 268, 289, 310]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g01_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 206\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 95, 109, 123, 136, 150, 164, 177, 191, 205]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g07_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 150\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139, 149]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 299\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 19, 39, 59, 79, 99, 119, 139, 158, 178, 198, 218, 238, 258, 278, 298]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g18_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 181\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 180]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g09_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g18_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 181\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 180]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g19_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g19_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g22_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 219\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 29, 43, 58, 72, 87, 101, 116, 130, 145, 159, 174, 188, 203, 218]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g22_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 339\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 22, 45, 67, 90, 112, 135, 157, 180, 202, 225, 247, 270, 292, 315, 338]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g16_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g02_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g14_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 157\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 31, 41, 52, 62, 72, 83, 93, 104, 114, 124, 135, 145, 156]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g17_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 354\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 23, 47, 70, 94, 117, 141, 164, 188, 211, 235, 258, 282, 305, 329, 353]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g04_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 213\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 42, 56, 70, 84, 98, 113, 127, 141, 155, 169, 183, 197, 212]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g15_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g15_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g07_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g06_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 235\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 46, 62, 78, 93, 109, 124, 140, 156, 171, 187, 202, 218, 234]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 207\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 96, 109, 123, 137, 151, 164, 178, 192, 206]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g08_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 201\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 40, 53, 66, 80, 93, 106, 120, 133, 146, 160, 173, 186, 200]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g10_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g13_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 239\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 126, 142, 158, 174, 190, 206, 222, 238]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g15_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 358\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 23, 47, 71, 95, 119, 142, 166, 190, 214, 238, 261, 285, 309, 333, 357]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g10_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g24_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 213\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 42, 56, 70, 84, 98, 113, 127, 141, 155, 169, 183, 197, 212]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g10_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g05_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 163\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 21, 32, 43, 54, 64, 75, 86, 97, 108, 118, 129, 140, 151, 162]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g04_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g05_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 180\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167, 179]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g17_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 204\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 40, 54, 67, 81, 94, 108, 121, 135, 148, 162, 175, 189, 203]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g24_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 167\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 121, 132, 143, 154, 166]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g01_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g24_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 209\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 55, 69, 83, 97, 110, 124, 138, 152, 166, 180, 194, 208]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g23_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 447\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 29, 59, 89, 118, 148, 178, 208, 237, 267, 297, 327, 356, 386, 416, 446]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g19_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 151\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g07_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 192\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 25, 38, 50, 63, 76, 89, 101, 114, 127, 140, 152, 165, 178, 191]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 324\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 21, 43, 64, 86, 107, 129, 150, 172, 193, 215, 236, 258, 279, 301, 323]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g22_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 206\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 95, 109, 123, 136, 150, 164, 177, 191, 205]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g11_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 152\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 151]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g11_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 210\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 55, 69, 83, 97, 111, 125, 139, 153, 167, 181, 195, 209]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g23_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "  ğŸ“¸ Frames read so far: 450\n",
      "  ğŸ“¸ Frames read so far: 475\n",
      "  ğŸ“¸ Frames read so far: 500\n",
      "  ğŸ“¸ Frames read so far: 525\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 535\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 35, 71, 106, 142, 178, 213, 249, 284, 320, 356, 391, 427, 462, 498, 534]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g07_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 159\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 10, 21, 31, 42, 52, 63, 73, 84, 94, 105, 115, 126, 136, 147, 158]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g01_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g15_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g05_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g18_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g14_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g13_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 201\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 40, 53, 66, 80, 93, 106, 120, 133, 146, 160, 173, 186, 200]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g02_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 206\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 54, 68, 82, 95, 109, 123, 136, 150, 164, 177, 191, 205]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g18_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g03_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g16_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 199\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 39, 52, 66, 79, 92, 105, 118, 132, 145, 158, 171, 184, 198]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g12_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 229\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 45, 60, 76, 91, 106, 121, 136, 152, 167, 182, 197, 212, 228]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g04_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g12_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g17_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g15_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g16_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g16_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g06_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 228\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 45, 60, 75, 90, 105, 121, 136, 151, 166, 181, 196, 211, 227]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g21_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g03_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g19_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 203\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 40, 53, 67, 80, 94, 107, 121, 134, 148, 161, 175, 188, 202]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g07_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 233\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 46, 61, 77, 92, 108, 123, 139, 154, 170, 185, 201, 216, 232]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g01_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g11_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g06_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g04_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g08_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g20_c07.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g05_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g14_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g25_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ“¦ Stacking videos and labels into tensors\n",
      "âœ… Split loaded successfully !! \n",
      "ğŸ“ Videos tensor shape: torch.Size([106, 16, 3, 224, 224])\n",
      "ğŸ·ï¸  Labels tensor shape: torch.Size([106])\n",
      "ğŸ“„ ==================================================\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "ğŸ“‚ Loading TESTING data\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“„ Loading split file\n",
      "ğŸ“ Split file path: /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/splits/test.txt\n",
      "ğŸ“„ ==================================================\n",
      "ğŸ“‘ Total entries found in split file: 24\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "  ğŸ“¸ Frames read so far: 450\n",
      "  ğŸ“¸ Frames read so far: 475\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 482\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448, 481]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g17_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 214\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 42, 56, 71, 85, 99, 113, 127, 142, 156, 170, 184, 198, 213]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g16_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "  ğŸ“¸ Frames read so far: 350\n",
      "  ğŸ“¸ Frames read so far: 375\n",
      "  ğŸ“¸ Frames read so far: 400\n",
      "  ğŸ“¸ Frames read so far: 425\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 446\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 29, 59, 89, 118, 148, 178, 207, 237, 267, 296, 326, 356, 385, 415, 445]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g25_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 179\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 23, 35, 47, 59, 71, 83, 94, 106, 118, 130, 142, 154, 166, 178]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g13_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 324\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 21, 43, 64, 86, 107, 129, 150, 172, 193, 215, 236, 258, 279, 301, 323]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_1_Basketball/v_Basketball_g21_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 182\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 181]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g07_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 300\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 19, 39, 59, 79, 99, 119, 139, 159, 179, 199, 219, 239, 259, 279, 299]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g18_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 209\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 27, 41, 55, 69, 83, 97, 110, 124, 138, 152, 166, 180, 194, 208]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g20_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "  ğŸ“¸ Frames read so far: 250\n",
      "  ğŸ“¸ Frames read so far: 275\n",
      "  ğŸ“¸ Frames read so far: 300\n",
      "  ğŸ“¸ Frames read so far: 325\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 340\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 22, 45, 67, 90, 113, 135, 158, 180, 203, 226, 248, 271, 293, 316, 339]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g21_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g02_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 239\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 126, 142, 158, 174, 190, 206, 222, 238]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g14_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 169\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 11, 22, 33, 44, 56, 67, 78, 89, 100, 112, 123, 134, 145, 156, 168]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g08_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_2_Biking/v_Biking_g09_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 196\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 13, 26, 39, 52, 65, 78, 91, 104, 117, 130, 143, 156, 169, 182, 195]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g07_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g08_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 228\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 30, 45, 60, 75, 90, 105, 121, 136, 151, 166, 181, 196, 211, 227]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g23_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 237\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 62, 78, 94, 110, 125, 141, 157, 173, 188, 204, 220, 236]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g14_c03.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 216\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 14, 28, 43, 57, 71, 86, 100, 114, 129, 143, 157, 172, 186, 200, 215]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g11_c05.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g09_c06.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 239\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 126, 142, 158, 174, 190, 206, 222, 238]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g17_c02.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g24_c04.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ¬ ==================================================\n",
      "ğŸ¥ Loading video\n",
      "ğŸ“ Video path      : /home/jovyan/va-dataset/Student_2024ab05275_Video_Classification/dataset/class_3_WalkingWithDog/v_WalkingWithDog_g17_c01.avi\n",
      "ğŸ§® Target #frames  : 16\n",
      "ğŸ¬ ==================================================\n",
      "  ğŸ“¸ Frames read so far: 25\n",
      "  ğŸ“¸ Frames read so far: 50\n",
      "  ğŸ“¸ Frames read so far: 75\n",
      "  ğŸ“¸ Frames read so far: 100\n",
      "  ğŸ“¸ Frames read so far: 125\n",
      "  ğŸ“¸ Frames read so far: 150\n",
      "  ğŸ“¸ Frames read so far: 175\n",
      "  ğŸ“¸ Frames read so far: 200\n",
      "  ğŸ“¸ Frames read so far: 225\n",
      "â¹ï¸  End of video reached or frame read failed\n",
      "âœ… Total frames extracted: 240\n",
      "ğŸ“ Applying uniform temporal sampling\n",
      "ğŸ”¢ Sampled frame indices: [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239]\n",
      "ğŸ§© Frames after temporal processing: 16\n",
      "ğŸ¨ Applying ImageNet normalization & transforms\n",
      "ğŸ“¦ Final video tensor shape: torch.Size([16, 3, 224, 224])\n",
      "ğŸ¬ ==================================================\n",
      "\n",
      "\n",
      "ğŸ“¦ Stacking videos and labels into tensors\n",
      "âœ… Split loaded successfully !! \n",
      "ğŸ“ Videos tensor shape: torch.Size([24, 16, 3, 224, 224])\n",
      "ğŸ·ï¸  Labels tensor shape: torch.Size([24])\n",
      "ğŸ“„ ==================================================\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "âœ… Dataset summary\n",
      "-----------------------------------------------\n",
      "\n",
      "ğŸ“ Training videos : 106\n",
      "ğŸ§ª Testing videos  : 24\n",
      "ğŸ·ï¸  Total classes  : 3\n",
      "ğŸ“ Train tensor    : torch.Size([106, 16, 3, 224, 224])  (N, T, C, H, W)\n",
      "ğŸ“ Test tensor     : torch.Size([24, 16, 3, 224, 224])   (N, T, C, H, W)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# LOAD DATA USING OFFICIAL TRAIN / TEST SPLITS\n",
    "# ==========================================================\n",
    "# This section loads videos and labels using pre-defined\n",
    "# split files (e.g., train.txt, test.txt).\n",
    "#\n",
    "# Each split file is expected to contain relative paths\n",
    "# to video files, one per line, such as:\n",
    "#\n",
    "#   class_0/video_001.avi\n",
    "#   class_1/video_023.avi\n",
    "#\n",
    "# The parent folder name (class_*) is used as the label.\n",
    "# ==========================================================\n",
    "\n",
    "def load_split(split_file):\n",
    "    \"\"\"\n",
    "    Load videos and labels from a split file.\n",
    "\n",
    "    Args:\n",
    "        split_file (Path): Path to the split text file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]:\n",
    "            - videos: Tensor of shape (N, T, C, H, W)\n",
    "            - labels: Tensor of shape (N,)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nğŸ“„ ==================================================\")\n",
    "    print(\"ğŸ“„ Loading split file\")\n",
    "    print(f\"ğŸ“ Split file path: {split_file}\")\n",
    "    print(\"ğŸ“„ ==================================================\")\n",
    "\n",
    "    videos = []  # Will store per-video tensors\n",
    "    labels = []  # Will store integer class labels\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 1: Read split file\n",
    "    # --------------------------------------------------\n",
    "    with open(split_file, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    print(f\"ğŸ“‘ Total entries found in split file: {len(lines)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 2: Process each video entry\n",
    "    # --------------------------------------------------\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Construct absolute video path\n",
    "        video_path = DATASET_ROOT / line\n",
    "\n",
    "        # Extract class name from path\n",
    "        # Example: \"class_2/video_003.avi\" â†’ \"class_2\"\n",
    "        class_name = line.split(\"/\")[0]\n",
    "\n",
    "        # Map class name to integer label\n",
    "        label = CLASS_TO_IDX[class_name]\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ¬ ----------------------------------------------\")\n",
    "        print(f\"ğŸ¥ Processing video {idx + 1}/{len(lines)}\")\n",
    "        print(f\"ğŸ“ Relative path : {line}\")\n",
    "        print(f\"ğŸ“ Absolute path : {video_path}\")\n",
    "        print(f\"ğŸ·ï¸  Class name   : {class_name}\")\n",
    "        print(f\"ğŸ”¢ Class index  : {label}\")\n",
    "        print(\"ğŸ¬ ----------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Load and preprocess video (T, C, H, W)\n",
    "        video_tensor = load_video(video_path)\n",
    "\n",
    "        # Append video tensor and label\n",
    "        videos.append(video_tensor)\n",
    "        labels.append(label)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 3: Stack videos and labels into tensors\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nğŸ“¦ Stacking videos and labels into tensors\")\n",
    "\n",
    "    videos_tensor = torch.stack(videos)        # Shape: (N, T, C, H, W)\n",
    "    labels_tensor = torch.tensor(labels)       # Shape: (N,)\n",
    "\n",
    "    print(\"âœ… Split loaded successfully !! \")\n",
    "    print(f\"ğŸ“ Videos tensor shape: {videos_tensor.shape}\")\n",
    "    print(f\"ğŸ·ï¸  Labels tensor shape: {labels_tensor.shape}\")\n",
    "    print(\"ğŸ“„ ==================================================\\n\")\n",
    "\n",
    "    return videos_tensor, labels_tensor\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD TRAINING DATA\n",
    "# ==========================================================\n",
    "print(\"\\n-----------------------------------------------\")\n",
    "print(\"ğŸ“‚ Loading TRAINING data\")\n",
    "print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "X_train, y_train = load_split(SPLITS_DIR / \"train.txt\")\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD TESTING DATA\n",
    "# ==========================================================\n",
    "print(\"\\n-----------------------------------------------\")\n",
    "print(\"ğŸ“‚ Loading TESTING data\")\n",
    "print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "X_test, y_test = load_split(SPLITS_DIR / \"test.txt\")\n",
    "\n",
    "# ==========================================================\n",
    "# DATASET SUMMARY\n",
    "# ==========================================================\n",
    "print(\"\\n-----------------------------------------------\")\n",
    "print(\"âœ… Dataset summary\")\n",
    "print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "print(f\"ğŸ“ Training videos : {len(X_train)}\")\n",
    "print(f\"ğŸ§ª Testing videos  : {len(X_test)}\")\n",
    "print(f\"ğŸ·ï¸  Total classes  : {NUM_CLASSES}\")\n",
    "print(f\"ğŸ“ Train tensor    : {X_train.shape}  (N, T, C, H, W)\")\n",
    "print(f\"ğŸ“ Test tensor     : {X_test.shape}   (N, T, C, H, W)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ca61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# VIDEO DATASET WITH OPTIONAL DATA AUGMENTATION\n",
    "# ==========================================================\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for video classification.\n",
    "\n",
    "    Each sample consists of:\n",
    "    - A video: sequence of frames (Tensor)\n",
    "    - A label: class index or class name\n",
    "\n",
    "    Data augmentation:\n",
    "    - Random horizontal flip is applied\n",
    "      ONLY when train=True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: List[torch.Tensor],\n",
    "        y: List[int],\n",
    "        train: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            X (List[Tensor]):\n",
    "                List of videos.\n",
    "                Each video is a Tensor of shape:\n",
    "                (num_frames, channels, height, width)\n",
    "\n",
    "            y (List[int]):\n",
    "                Corresponding labels for each video.\n",
    "\n",
    "            train (bool):\n",
    "                If True:\n",
    "                    - Apply data augmentation (horizontal flip)\n",
    "                If False:\n",
    "                    - No augmentation (used for validation/testing)\n",
    "        \"\"\"\n",
    "        # Store videos\n",
    "        self.X = X\n",
    "\n",
    "        # Store labels\n",
    "        self.y = y\n",
    "\n",
    "        # Flag to control augmentation behavior\n",
    "        self.train = train\n",
    "\n",
    "        # Define spatial augmentation:\n",
    "        # Randomly flips an image horizontally with 50% probability\n",
    "        #\n",
    "        # IMPORTANT:\n",
    "        # - This does NOT add new pixels\n",
    "        # - It only rearranges existing pixels\n",
    "        self.flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "\n",
    "        This method is required by PyTorch's Dataset class\n",
    "        so that DataLoader knows how many samples exist.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Retrieve one sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int):\n",
    "                Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, int]:\n",
    "                - video: Tensor of shape\n",
    "                  (num_frames, channels, height, width)\n",
    "                - label: corresponding class label\n",
    "        \"\"\"\n",
    "        # Fetch the video at the given index\n",
    "        video = self.X[idx]\n",
    "\n",
    "        # Apply data augmentation ONLY during training\n",
    "        if self.train:\n",
    "            # Apply horizontal flip independently to each frame\n",
    "            #\n",
    "            # Why per-frame?\n",
    "            # - Each frame is treated as an image\n",
    "            # - Maintains temporal order\n",
    "            # - Simple and effective spatial augmentation\n",
    "            #\n",
    "            # torch.stack is used to reconstruct the video\n",
    "            # back into a single Tensor\n",
    "            video = torch.stack([self.flip(frame) for frame in video])\n",
    "\n",
    "        # Return video and its label\n",
    "        return video, self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaf95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data loaders initialized\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# DATALOADERS\n",
    "# ==========================================================\n",
    "train_loader = DataLoader(\n",
    "    VideoDataset(X_train, y_train, train=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    VideoDataset(X_test, y_test, train=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Data loaders initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af423aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# TRAINING & EVALUATION FUNCTION\n",
    "# ==========================================================\n",
    "def train_and_evaluate(model, model_name):\n",
    "    \"\"\"\n",
    "    Train a deep learning model and evaluate its performance.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Creates result directories for storing outputs\n",
    "    2. Trains the model for a fixed number of epochs\n",
    "    3. Records training time\n",
    "    4. Evaluates the model on a test set\n",
    "    5. Computes classification metrics\n",
    "    6. Saves training loss plots and confusion matrices\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module):\n",
    "            The neural network model to train and evaluate\n",
    "        model_name (str):\n",
    "            Descriptive name of the model for logging and file naming\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            Dictionary containing performance metrics and confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Create directory structure for storing results\n",
    "    # ------------------------------------------------------\n",
    "    # Base directory for all experiment outputs\n",
    "    base_dir = PROJECT_ROOT / \"results\"\n",
    "    \n",
    "    # Subdirectories for different result types\n",
    "    cm_dir = base_dir / \"confusion_matrices\"\n",
    "    perf_dir = base_dir / \"performance_plots\"\n",
    "    feat_dir = base_dir / \"feature_visualizations\"\n",
    "    \n",
    "    # Create directories if they do not already exist\n",
    "    cm_dir.mkdir(parents=True, exist_ok=True)\n",
    "    perf_dir.mkdir(parents=True, exist_ok=True)\n",
    "    feat_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Model setup and initialization\n",
    "    # ------------------------------------------------------\n",
    "    print(f\"\\nğŸš€ Training {model_name}\")\n",
    "\n",
    "    # Move the model to the appropriate device (CPU / GPU)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Optimizer and loss function definition\n",
    "    # ------------------------------------------------------\n",
    "    # Adam optimizer is chosen for faster convergence and stability\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=1e-4  # L2 regularization to reduce overfitting\n",
    "    )\n",
    "\n",
    "    # Cross-entropy loss is standard for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Training phase\n",
    "    # ------------------------------------------------------\n",
    "    # Switch model to training mode\n",
    "    # Enables dropout and batch normalization updates\n",
    "    model.train()\n",
    "\n",
    "    # Record the start time of training\n",
    "    start_train = time.time()\n",
    "\n",
    "    # List to store average loss per epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Iterate over training epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # Accumulator for total loss in the epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over mini-batches from the training loader\n",
    "        for videos, labels in train_loader:\n",
    "\n",
    "            # Move input data and labels to device\n",
    "            videos = videos.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # Clear gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute model outputs\n",
    "            outputs = model(videos)\n",
    "\n",
    "            # Compute loss between predictions and ground truth\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        # Log training progress\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{EPOCHS}] \"\n",
    "            f\"- Avg Loss: {avg_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Compute total training time\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Save training loss curve\n",
    "    # ------------------------------------------------------\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_losses, marker=\"o\")\n",
    "    plt.title(f\"{model_name} - Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Generate file-safe model name\n",
    "    loss_plot_path = perf_dir / f\"{model_name.replace(' ', '_')}_loss.png\"\n",
    "\n",
    "    # Save plot to disk\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Evaluation phase (Inference)\n",
    "    # ------------------------------------------------------\n",
    "    # Switch model to evaluation mode\n",
    "    # Disables dropout and freezes batch normalization\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store predictions and ground-truth labels\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    # Record inference start time\n",
    "    start_inf = time.time()\n",
    "\n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in test_loader:\n",
    "\n",
    "            # Move input videos to device\n",
    "            videos = videos.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(videos)\n",
    "\n",
    "            # Convert logits to predicted class indices\n",
    "            batch_preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            preds.extend(batch_preds.cpu().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "\n",
    "    # Compute average inference time per video\n",
    "    inf_time = (time.time() - start_inf) / len(targets)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Metric computation\n",
    "    # ------------------------------------------------------\n",
    "    metrics = {\n",
    "        # Overall classification accuracy\n",
    "        \"accuracy\": accuracy_score(targets, preds),\n",
    "\n",
    "        # Precision averaged equally across classes\n",
    "        \"precision\": precision_score(targets, preds, average=\"macro\"),\n",
    "\n",
    "        # Recall averaged equally across classes\n",
    "        \"recall\": recall_score(targets, preds, average=\"macro\"),\n",
    "\n",
    "        # Harmonic mean of precision and recall\n",
    "        \"f1\": f1_score(targets, preds, average=\"macro\"),\n",
    "\n",
    "        # Total training duration in seconds\n",
    "        \"train_time\": train_time,\n",
    "\n",
    "        # Average inference time per sample\n",
    "        \"inf_time\": inf_time,\n",
    "\n",
    "        # Confusion matrix for class-wise error analysis\n",
    "        \"cm\": confusion_matrix(targets, preds)\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Print evaluation results\n",
    "    # ------------------------------------------------------\n",
    "    print(f\"\\nğŸ“Š {model_name} Performance\")\n",
    "    for key, value in metrics.items():\n",
    "        if key != \"cm\":\n",
    "            print(\n",
    "                f\"{key.replace('_', ' ').title():<20}: \"\n",
    "                f\"{value:.4f}\"\n",
    "            )\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Save confusion matrix visualization\n",
    "    # ------------------------------------------------------\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        metrics[\"cm\"],\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\"\n",
    "    )\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Generate file-safe path for confusion matrix\n",
    "    cm_path = cm_dir / f\"{model_name.replace(' ', '_')}_cm.png\"\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe317536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training 2D CNN + Temporal Pooling\n",
      "Epoch [1/5] - Avg Loss: 1.0642\n",
      "Epoch [2/5] - Avg Loss: 0.7528\n",
      "Epoch [3/5] - Avg Loss: 0.6352\n",
      "Epoch [4/5] - Avg Loss: 0.6067\n",
      "Epoch [5/5] - Avg Loss: 0.3910\n",
      "\n",
      "ğŸ“Š 2D CNN + Temporal Pooling Performance\n",
      "Accuracy            : 0.7500\n",
      "Precision           : 0.7937\n",
      "Recall              : 0.7593\n",
      "F1                  : 0.7653\n",
      "Train Time          : 15.4535\n",
      "Inf Time            : 0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R2Plus1D_18_Weights.KINETICS400_V1`. You can also use `weights=R2Plus1D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training 3D CNN (R(2+1)D)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Avg Loss: 0.7566\n",
      "Epoch [2/5] - Avg Loss: 0.2393\n",
      "Epoch [3/5] - Avg Loss: 0.1647\n",
      "Epoch [4/5] - Avg Loss: 0.1303\n",
      "Epoch [5/5] - Avg Loss: 0.2494\n",
      "\n",
      "ğŸ“Š 3D CNN (R(2+1)D) Performance\n",
      "Accuracy            : 0.9583\n",
      "Precision           : 0.9524\n",
      "Recall              : 0.9630\n",
      "F1                  : 0.9548\n",
      "Train Time          : 71.0993\n",
      "Inf Time            : 0.0193\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# RUN EXPERIMENTS\n",
    "# ==========================================================\n",
    "metrics_2d = train_and_evaluate(\n",
    "    CNN2DTemporal(NUM_CLASSES),\n",
    "    \"2D CNN + Temporal Pooling\"\n",
    ")\n",
    "\n",
    "metrics_3d = train_and_evaluate(\n",
    "    CNN3D(NUM_CLASSES),\n",
    "    \"3D CNN (R(2+1)D)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb1a942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“ˆ DETAILED MODEL COMPARISON REPORT\n",
      "============================================================\n",
      "\n",
      "ğŸ”¹ Accuracy Comparison\n",
      "2D CNN Accuracy : 0.7500\n",
      "3D CNN Accuracy : 0.9583\n",
      "\n",
      "ğŸ”¹ Computational Efficiency\n",
      "2D CNN Training Time (s)        : 15.45\n",
      "3D CNN Training Time (s)        : 71.10\n",
      "2D CNN Inference Time / Video(s): 0.0106\n",
      "3D CNN Inference Time / Video(s): 0.0193\n",
      "\n",
      "ğŸ”¹ Qualitative Analysis\n",
      "âœ” The 3D CNN outperforms the 2D CNN by explicitly modeling spatiotemporal patterns, making it more suitable for complex actions.\n",
      "\n",
      "ğŸ”¹ Final Conclusion:\n",
      "2D CNNs provide a strong baseline with efficient inference, while 3D CNNs offer improved performance at the cost of higher computation.\n",
      "\n",
      "âœ… Experiment completed successfully\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# DETAILED COMPARISON REPORT\n",
    "# ==========================================================\n",
    "\n",
    "def compare_models(metrics_2d, metrics_3d):\n",
    "    \"\"\"\n",
    "    Generate comparison charts and a detailed report\n",
    "    for 2D CNN vs 3D CNN models.\n",
    "\n",
    "    Args:\n",
    "        metrics_2d (dict): Metrics for 2D CNN\n",
    "        metrics_3d (dict): Metrics for 3D CNN\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Sanity checks\n",
    "    # ------------------------------------------------------\n",
    "    required_keys = {\"accuracy\", \"train_time\", \"inf_time\"}\n",
    "\n",
    "    if not required_keys.issubset(metrics_2d):\n",
    "        raise ValueError(\"metrics_2d missing required keys\")\n",
    "\n",
    "    if not required_keys.issubset(metrics_3d):\n",
    "        raise ValueError(\"metrics_3d missing required keys\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Extract metrics\n",
    "    # ------------------------------------------------------\n",
    "    models = [\"2D CNN\", \"3D CNN\"]\n",
    "\n",
    "    accuracy = [\n",
    "        metrics_2d[\"accuracy\"],\n",
    "        metrics_3d[\"accuracy\"]\n",
    "    ]\n",
    "\n",
    "    train_time = [\n",
    "        metrics_2d[\"train_time\"],\n",
    "        metrics_3d[\"train_time\"]\n",
    "    ]\n",
    "\n",
    "    inf_time = [\n",
    "        metrics_2d[\"inf_time\"],\n",
    "        metrics_3d[\"inf_time\"]\n",
    "    ]\n",
    "\n",
    "    # ======================================================\n",
    "    # CHART 1: Accuracy Comparison\n",
    "    # ======================================================\n",
    "    plt.figure()\n",
    "    plt.bar(models, accuracy)\n",
    "    plt.title(\"Model Accuracy Comparison\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    # ======================================================\n",
    "    # CHART 2: Training Time Comparison\n",
    "    # ======================================================\n",
    "    plt.figure()\n",
    "    plt.bar(models, train_time)\n",
    "    plt.title(\"Training Time Comparison\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ======================================================\n",
    "    # CHART 3: Inference Time Comparison\n",
    "    # ======================================================\n",
    "    plt.figure()\n",
    "    plt.bar(models, inf_time)\n",
    "    plt.title(\"Inference Time per Video Comparison\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ======================================================\n",
    "    # TEXTUAL COMPARISON REPORT\n",
    "    # ======================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“ˆ DETAILED MODEL COMPARISON REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Accuracy\n",
    "    print(\"\\nğŸ”¹ Accuracy Comparison\")\n",
    "    print(f\"2D CNN Accuracy : {metrics_2d['accuracy']:.4f}\")\n",
    "    print(f\"3D CNN Accuracy : {metrics_3d['accuracy']:.4f}\")\n",
    "\n",
    "    # Efficiency\n",
    "    print(\"\\nğŸ”¹ Computational Efficiency\")\n",
    "    print(f\"2D CNN Training Time (s)        : {metrics_2d['train_time']:.2f}\")\n",
    "    print(f\"3D CNN Training Time (s)        : {metrics_3d['train_time']:.2f}\")\n",
    "    print(f\"2D CNN Inference Time / Video(s): {metrics_2d['inf_time']:.4f}\")\n",
    "    print(f\"3D CNN Inference Time / Video(s): {metrics_3d['inf_time']:.4f}\")\n",
    "\n",
    "    # Qualitative analysis\n",
    "    print(\"\\nğŸ”¹ Qualitative Analysis\")\n",
    "    if metrics_3d[\"accuracy\"] > metrics_2d[\"accuracy\"]:\n",
    "        print(\n",
    "            \"âœ” The 3D CNN outperforms the 2D CNN by explicitly modeling \"\n",
    "            \"spatiotemporal patterns, making it more suitable for complex actions.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"âœ” The 2D CNN achieves competitive accuracy with significantly lower \"\n",
    "            \"computational cost, making it suitable for real-time applications.\"\n",
    "        )\n",
    "\n",
    "    # Final conclusion\n",
    "    print(\n",
    "        \"\\nğŸ”¹ Final Conclusion:\\n\"\n",
    "        \"2D CNNs provide a strong baseline with efficient inference, while \"\n",
    "        \"3D CNNs offer improved performance at the cost of higher computation.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ… Experiment completed successfully\")\n",
    "\n",
    "compare_models(metrics_2d, metrics_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bd8d3-145d-4563-add2-edb0e37b0bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
